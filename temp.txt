#include <uvgrtp/lib.hh>

#include <iostream>
#include <cstring>

/* RTP is a protocol for real-time streaming. The simplest usage
 * scenario is sending one RTP stream and receiving it. This example
 * Shows how to send one RTP stream. These examples perform a simple
 * test if they are run. You may run the receiving examples at the same
 * time to see the whole demo. */

/* parameters of this example. You may change these to reflect
 * you network environment. */
constexpr char REMOTE_ADDRESS[] = "127.0.0.1";
constexpr uint16_t REMOTE_PORT = 8890;

// the parameters of demostration
constexpr size_t PAYLOAD_LEN = 100;
constexpr int    AMOUNT_OF_TEST_PACKETS = 100;
constexpr auto   END_WAIT = std::chrono::seconds(5);

int main(void)
{
    std::cout << "Starting uvgRTP RTP sending example" << std::endl;

    /* To use the library, one must create a global RTP context object */
    uvgrtp::context ctx;

    // A session represents
    uvgrtp::session *sess = ctx.create_session(REMOTE_ADDRESS);

    /* Each RTP session has one or more media streams. These media streams are bidirectional
     * and they require both source and destination ports for the connection. One must also
     * specify the media format for the stream and any configuration flags if needed.
     *
     * See Configuration example for more details about configuration.
     *
     * First port is source port aka the port that we listen to and second port is the port
     * that remote listens to
     *
     * This same object is used for both sending and receiving media
     *
     * In this example, we have one media stream with the remote participant: H265 */

    int flags = RCE_SEND_ONLY;
    uvgrtp::media_stream *hevc = sess->create_stream(REMOTE_PORT, RTP_FORMAT_H265, flags);

    if (hevc)
    {
        /* In this example we send packets as fast as possible. The source can be
         * a file or a real-time encoded stream */
        for (int i = 0; i < AMOUNT_OF_TEST_PACKETS; ++i)
        {
            std::unique_ptr<uint8_t[]> dummy_frame = std::unique_ptr<uint8_t[]>(new uint8_t[PAYLOAD_LEN]);
            memset(dummy_frame.get(), 'a', PAYLOAD_LEN); // NAL payload
            memset(dummy_frame.get(),     0, 3);
            memset(dummy_frame.get() + 3, 1, 1);
            memset(dummy_frame.get() + 4, 1, (19 << 1)); // Intra frame NAL type

            if ((i+1)%10  == 0 || i == 0) // print every 10 frames and first
                std::cout << "Sending frame " << i + 1 << '/' << AMOUNT_OF_TEST_PACKETS << std::endl;

            if (hevc->push_frame(std::move(dummy_frame), PAYLOAD_LEN, RTP_NO_FLAGS) != RTP_OK)
            {
                std::cout << "Failed to send RTP frame!" << std::endl;
            }
        }

         std::cout << "Sending finished. Waiting "<< END_WAIT.count()
                   << " seconds before exiting." << std::endl;

        // wait a little bit so pop-up console users have time to see the results
        std::this_thread::sleep_for(END_WAIT);

        sess->destroy_stream(hevc);
    }

    if (sess)
    {
        /* Session must be destroyed manually */
        ctx.destroy_session(sess);
    }

    return EXIT_SUCCESS;
}
上記のコードは、uvgrtpを使用して、RTPで送信をおこなうサンプルコードです。

#include <dlfcn.h>
#include <iostream>
#include <cassert>
#include <cstring>
#include <fstream>
#include <opencv2/core.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/imgproc.hpp>
#include <opencv2/videoio.hpp>
#include "/Users/takeru/Downloads/openh264-2.3.1/codec/api/wels/codec_api.h"
#include "/Users/takeru/Documents/GitHub/uvgRTP/include/uvgrtp/lib.hh"

#include <opencv2/core.hpp>
#include <opencv2/imgcodecs.hpp>
#include <opencv2/imgproc.hpp>

using namespace std;
using namespace cv;

constexpr char REMOTE_ADDRESS[] = "127.0.0.1";
constexpr uint16_t REMOTE_PORT = 12345;

// the parameters of demostration
constexpr size_t PAYLOAD_LEN = 100;
constexpr int AMOUNT_OF_TEST_PACKETS = 100;
constexpr auto END_WAIT = std::chrono::seconds(5);

int main()
{
    uvgrtp::context ctx;

    // ライブラリの読み込み
    void *handle = dlopen("../libopenh264-2.3.1-mac-arm64.dylib", RTLD_LAZY);
    if (!handle)
    {
        cerr << "ライブラリの読み込みに失敗: " << dlerror() << endl;
        return -1;
    }

    // エンコーダの作成
    typedef int (*CreateEncoderFunc)(ISVCEncoder **);
    CreateEncoderFunc createEncoder = (CreateEncoderFunc)dlsym(handle, "WelsCreateSVCEncoder");
    ISVCEncoder *encoder = nullptr;
    if (createEncoder(&encoder) != 0 || encoder == nullptr)
    {
        cerr << "エンコーダの作成に失敗" << endl;
        return -1;
    }

    // エンコーダの初期化パラメータ
    SEncParamBase param;
    memset(&param, 0, sizeof(SEncParamBase));
    param.iUsageType = CAMERA_VIDEO_REAL_TIME;
    param.fMaxFrameRate = 30;       // 一般的なフレームレート
    param.iPicWidth = 640;          // 幅
    param.iPicHeight = 480;         // 高さ
    param.iTargetBitrate = 5000000; // ビットレート
    if (encoder->Initialize(&param) != 0)
    {
        cerr << "エンコーダの初期化に失敗" << endl;
        return -1;
    }

    // カメラの初期化
    VideoCapture cap(1);
    if (!cap.isOpened())
    {
        cerr << "カメラが開けません" << endl;
        return -1;
    }

    Mat frame;
    int frameCount = 0;
    int captureDuration = 5; // 秒単位
    ofstream outFile("output.264", ios::out | ios::binary);

    while (frameCount < captureDuration * 30)
    {
        // 30FPSでエンコード
        cap >> frame;
        if (frame.empty())
            break;

        Mat yuv;
        // YUVフォーマットに変換
        cvtColor(frame, yuv, COLOR_BGR2YUV_I420);

        SSourcePicture pic;
        memset(&pic, 0, sizeof(SSourcePicture));
        pic.iPicWidth = frame.cols;
        pic.iPicHeight = frame.rows;
        pic.iColorFormat = videoFormatI420;
        pic.iStride[0] = frame.cols;
        pic.iStride[1] = pic.iStride[2] = frame.cols / 2;
        pic.pData[0] = yuv.data;
        pic.pData[1] = yuv.data + frame.cols * frame.rows;
        pic.pData[2] = pic.pData[1] + (frame.cols * frame.rows) / 4;

        SFrameBSInfo info;
        memset(&info, 0, sizeof(SFrameBSInfo));
        if (encoder->EncodeFrame(&pic, &info) != 0)
        {
            cerr << "エンコードに失敗" << endl;
            break;
        }

        // エンコードされたデータをファイルに書き込む
        for (int layer = 0; layer < info.iLayerNum; ++layer)
        {
            int layerSize = 0;
            SLayerBSInfo &layerInfo = info.sLayerInfo[layer];
            for (int nal = 0; nal < layerInfo.iNalCount; ++nal)
            {
                outFile.write(reinterpret_cast<char *>(layerInfo.pBsBuf + layerSize), layerInfo.pNalLengthInByte[nal]);
                layerSize += layerInfo.pNalLengthInByte[nal];
            }
        }
        frameCount++;
    }

    // エンコーダとファイルの解放
    encoder->Uninitialize();
    typedef void (*DestroyEncoderFunc)(ISVCEncoder *);
    DestroyEncoderFunc destroyEncoder = (DestroyEncoderFunc)dlsym(handle, "WelsDestroySVCEncoder");
    destroyEncoder(encoder);
    dlclose(handle);
    outFile.close();

    return 0;
}
また、上記のコードはopenh264を使用して、openCVからキャプチャしたカメラ映像をエンコードするコードです。

上記の2つのコードを参考にし、openCVでカメラから得たデータをh264でエンコードし、output.264として保存したものを、RTPで送信してください。